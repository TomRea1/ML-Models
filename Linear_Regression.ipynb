{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "First I'm going to implement linear regression in a for loop using the basic mathematical principals behind this linear model.\n",
        "\n",
        "First we take the weighted sum of the feature vector + 1 for the bias or intercept term. z = X*W + b\n",
        "\n",
        "Next we use the resultant prediction vector (z) and compare it to the actual labels in the mean square error formula. - MSE is conventional for linear regression, its not perfect but its fine for linearly separable data.\n",
        "\n",
        "Next we update the weights and bias term using gradient descent to determine the number by which to multiply the W / B term and mitigating it by the learning rate to prevent steps being too large or too small -> won't converge in this case.\n",
        "\n"
      ],
      "metadata": {
        "id": "V0YmApAuvh2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoWu-bH3vhKp"
      },
      "outputs": [],
      "source": [
        "\n",
        "some_data = whatever\n",
        "X = some_data[:, :-1]\n",
        "y = some_data[:, -1]\n",
        "\n",
        "initialise W randomly\n",
        "initialise b = 1\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Compute predictions\n",
        "    z = X * W + b\n",
        "\n",
        "    # Compute loss\n",
        "    print(f\"MSE at epoch {epoch} = {sum((z - y) ** 2) / len(y)}\")\n",
        "\n",
        "    # Compute gradients\n",
        "    dW = -(2/len(y)) * sum(X.T * (y - z))\n",
        "    db = -(2/len(y)) * sum(y - z)\n",
        "\n",
        "    # Update weights\n",
        "    W = W - learning_rate * dW\n",
        "    b = b - learning_rate * db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudo-Inverse\n",
        "\n",
        "Technically we don't have to implement regression like this. ***IF***  we are working with a small - medium size dataset, we can compute the optimal weights using the, 'normal equation,' for linear regression and the, 'pseudo-inverse.'\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "What we're actually doing above is calculating the weighted sum for each feature vector in the data set individually. We can calculate it for the entire dataset by computing X.T by X giving us a square matrix representing feature correlations and inverting it.\n",
        "\n",
        "We can then multiply that by X.T.dot(y) which essentially maps the target vector y, onto the feature space yielding the optimal weight vector.\n",
        "\n",
        "Note : this method only works if X.T.dot(X) can be inverted.\n",
        "\n",
        "```\n",
        "np.linalg.inv((X.T.dot(X))).dot(X.T).dot(y)\n",
        "```\n",
        "This method will work regardless although it should be noted that neither method is suitable for large data sets as computing the pseudo-inverse is quite expensivie O(n^3)\n",
        "\n",
        "```\n",
        "np.linalg.pinv(X).dot(y)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B2mf7WJdxx4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def psuedo_inverse(X, y):\n",
        "  X = np.concatenate((np.ones((X.shape[0],1)), X.reshape(X.shape[0], X.shape[1])), axis=1)\n",
        "  return np.linalg.pinv(X).dot(y)\n",
        "\n",
        "# Same regression as above assuming the one above converges and assuming the data is linearly separable\n",
        "weights = pseudo_inverse(X, y)\n",
        "print(weights)"
      ],
      "metadata": {
        "id": "5B2jUO_U01mx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}